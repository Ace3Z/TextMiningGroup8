{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence 1: \"I love apples\"\n",
    "\n",
    "**Explanation:**  \n",
    "In this sentence, VADER identifies the word \"love\" as strongly positive. The remaining words (\"I\" and \"apples\") are not associated with any sentiment in the lexicon. So, the overall sentiment is driven almost completely by \"love,\" resulting in a high positive score.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 2: \"I don't love apples\"\n",
    "\n",
    "**Explanation:**  \n",
    "The presence of the negation \"don't\" inverts the sentiment of the word \"love.\" VADER’s negation rule switches the polarity of a positive term to negative, leading to an overall negative sentiment for the sentence. This shows how even a single negator can significantly change the sentiment outcome.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 3: \"I love apples :-)\"\n",
    "\n",
    "**Explanation:**  \n",
    "The positive term \"love\" is again present, and the additional emoticon \":-)\" provides an extra boost. VADER is designed to recognize common emoticons and assign them sentiment values. The combination results in an even higher overall positive sentiment than the sentence without the emoticon.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 4: \"These houses are ruins\"\n",
    "\n",
    "**Explanation:**  \n",
    "The key term in this sentence is \"ruins,\" which in VADER’s lexicon carries a negative connotation. As a result, the sentence is interpreted as negative. However, the context is unknown, “ruins” could describe old structures as well but VADER does not perform context disambiguation, so it relies solely on the lexicon value.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 5: \"These houses are certainly not considered ruins\"\n",
    "\n",
    "**Explanation:**  \n",
    "In this sentence, the negative word \"ruins\" is modified by the negation \"not,\" which causes VADER to invert its sentiment. Additionally, the modifier \"certainly\" slightly improves the sentiment intensity. The inversion of a negative term leads VADER to interpret the sentence as leaning towards a positive sentiment, even though one might expect it to be more neutral.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 6: \"He lies in the chair in the garden\"\n",
    "\n",
    "**Explanation:**  \n",
    "The word \"lies\" is ambiguous. Although it can mean \"reclines\" in a neutral sense, VADER’s lexicon associates \"lies\" with dishonesty, a negative trait. Without the ability to disambiguate between the meanings, VADER assigns a negative sentiment to the sentence, even though the intended meaning is simply descriptive (and, thus, one would be expect the sentiment to be neutral).\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 7: \"This house is like any house\"\n",
    "\n",
    "**Explanation:**  \n",
    "In this case, the word \"like\" is interpreted by VADER as a positive signal. However, the sentence is meant to express a neutral comparison, that the house is unremarkable. Because VADER registers \"like\" with a positive bias, it results in a slight positive sentiment, which does not fully capture the neutral intent of the sentence.\n",
    "\n",
    "\n",
    "\n",
    "## **Final Outcome:**  \n",
    "VADER’s approach relies on a fixed lexicon and a set of heuristic rules. While it effectively captures clear positive and negative signals (handles negation and emoticons), its inability to disambiguate word meanings or interpret subtle contextual cues can lead to sentiment scores that do not always match the intended sentiment of the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "########################### imports below have been added manually for solving the questions in this NB\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'sentiment_label': 'neutral', 'text_of_tweet': 'Scientists discovered a new type of organism in the depths of the ocean.', 'tweet_url': 'manually created'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in list(my_tweets.items())[::-1]:\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **NB!** I don't exactly understand whether by \"explain which scores are most relevant and why\" they refer to the scores that VADER produces (neg, pos, neu, compound), or the scores from the classification report (f1, P, R, accuracy, etc.). Therefore, I explain both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse `run_vader` from ***Lab3.2*** *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', textual_unit) # change to textual_unit so the whole tweet is displayed\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        16\n",
      "     neutral       0.75      0.75      0.75        16\n",
      "    positive       0.82      1.00      0.90        18\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.83      0.84        50\n",
      "weighted avg       0.85      0.84      0.84        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(gold, all_vader_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(A) Quantitative Evaluation**\n",
    "\n",
    "!! **For balanced multi-class classification, the micro averages are equal to accuracy because both are computed using the total count of correct predictions divided by the total number of samples.**\n",
    "\n",
    "VADER analyzes the polarity of words and produces 4 different sentiment scores for each text input:\n",
    "* Positive (`pos`): The proportion of words that express a positive sentiment\n",
    "* Negative (`neg`): The proportion of words that express a negative sentiment\n",
    "* Neutral (`neu`): The proportion of words that are neutral / lack clear sentiment\n",
    "* Compound (`compound`): An aggregated sentiment score; ranges from -1 (extremely negative) to +1 (extremely positive)\n",
    "\n",
    "After producing those 4 scores, the final sentiment label of the tweet is assigned based on the `compound` score, as detailed in the `vader_output_to_label` function. Therefore, the `compound` score is the most relevant score for the final label classification, which is done according to the following scheme:\n",
    "* **`compound` > 0** → *positive* label\n",
    "* **`compound` < 0** → *negative* label\n",
    "* **`compound` = 0** → *neutral* label\n",
    "\n",
    "In order to evaluate the performence of the VADER classifier, we used the scikit-learn's classification report. We present and explain the insights it provides:\n",
    "1. **Precision** (how many tweets did VADER classify correctly out of all classifications for a certain category):\n",
    "* Negative (`1.00`) - VADER achieves perfect precision on negative examples (correctly predicts them 100% of the time)\n",
    "* Neutral (`0.75`) - some tweets misclassified as neutral were actually positive or negative\n",
    "* Positive (`0.82`) - overall high precision on positive tweets, however, *18%* of those classified as positive are being misclassified\n",
    "2. **Recall** (how many tweets did VADER classify correctly out of all the tweets in a category (as determined by the gold label)):\n",
    "* Negative (`0.75`) - some negative tweets were misclassified as neutral or positive\n",
    "* Neutral (`0.75`) - VADER manages to identify *75%* of actual neutral tweets\n",
    "* Positive (`1.00`) - all actual positive tweets were detected correctly\n",
    "3. **F1-score** (harmonic mean of precision and recall):\n",
    "* Negative (`0.86`) - strong performance in detecting negative tweets\n",
    "* Neutral (`0.75`) - decent performance in classifying neutral tweets. Across the three categories, VADER has the lowest score for neutral tweets\n",
    "* Positive (`0.90`) - highly effective at identifying positive tweets\n",
    "4. **Accuracy** (overall percentage of correct classifications):\n",
    "* `84%` - VADER correctly classified *84%* of all tweets\n",
    "5. **Macro average** & **Weighted average**\n",
    "* Macro average - the average of precision, recall, and F1-score across all classes, treating each class equally (no weights)\n",
    "* Weighted average - the average of precision, recall, and F1-score, weighted by the number of samples in each class\n",
    "* Macro and weighted averages both showed nearly identical scores, suggesting that there are no severe class imbalance issues, and the model has a good stable performance across all classes\n",
    "\n",
    "For evaluating the overall performance, F1-scores and accuracy are most relevant as they give a good general idea of how well the classifier works. Accuracy alone can be a misleading metric, as it does not give insight on the types of errors the model makes and is sensitive to class imbalance - when one category is much more frequent that the rest (this is not the case here), therefore, combining it with F1-score, we can get a good overview of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Summary\n",
      "--------------------------------------------------\n",
      "Positive misclassified: 0\n",
      "Negative misclassified: 4\n",
      "Neutral misclassified: 4\n",
      "**************************************************\n",
      "\n",
      "Misclassified Positive Tweets\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Misclassified Negative Tweets\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Tweet: The traffic today was unbearable. Took me whole two hours to get home.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The traffic today was unbearable. Took me whole two hours to get home.\n",
      "INPUT TO VADER ['the', 'traffic', 'today', 'be', 'unbearable', '.', 'take', 'I', 'whole', 'two', 'hour', 'to', 'get', 'home', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Tweet: The food at the restaurant was cold and tasteless. Never going back.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The food at the restaurant was cold and tasteless. Never going back.\n",
      "INPUT TO VADER ['the', 'food', 'at', 'the', 'restaurant', 'be', 'cold', 'and', 'tasteless', '.', 'never', 'go', 'back', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Tweet: The service at the cafe was slow and the coffee was cold.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The service at the cafe was slow and the coffee was cold.\n",
      "INPUT TO VADER ['the', 'service', 'at', 'the', 'cafe', 'be', 'slow', 'and', 'the', 'coffee', 'be', 'cold', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Tweet: The restaurant was overpriced and the food was mediocre.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The restaurant was overpriced and the food was mediocre.\n",
      "INPUT TO VADER ['the', 'restaurant', 'be', 'overprice', 'and', 'the', 'food', 'be', 'mediocre', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Misclassified Neutral Tweets\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Tweet: Reading about the latest tech innovations. There have been a lot of new developments.\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Reading about the latest tech innovations. There have been a lot of new developments.\n",
      "INPUT TO VADER ['read', 'about', 'the', 'late', 'tech', 'innovation', '.', 'there', 'have', 'be', 'a', 'lot', 'of', 'new', 'development', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'compound': 0.3818}\n",
      "**************************************************\n",
      "\n",
      "Tweet: Listening to a podcast about space exploration. So much new information...\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Listening to a podcast about space exploration. So much new information...\n",
      "INPUT TO VADER ['listen', 'to', 'a', 'podcast', 'about', 'space', 'exploration', '.', 'so', 'much', 'new', 'information', '...']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.2263}\n",
      "**************************************************\n",
      "\n",
      "Tweet: Would you like to watch the sunset at the park with me tonight?\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Would you like to watch the sunset at the park with me tonight?\n",
      "INPUT TO VADER ['would', 'you', 'like', 'to', 'watch', 'the', 'sunset', 'at', 'the', 'park', 'with', 'I', 'tonight', '?']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.3612}\n",
      "**************************************************\n",
      "\n",
      "Tweet: Listening to a podcast about productivity. It provides many useful tips.\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Listening to a podcast about productivity. It provides many useful tips.\n",
      "INPUT TO VADER ['listen', 'to', 'a', 'podcast', 'about', 'productivity', '.', 'it', 'provide', 'many', 'useful', 'tip', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.4404}\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "misclassified_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
    "misclassified_tweets = {\"positive\": [], \"negative\": [], \"neutral\": []}\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    if gold[i] != all_vader_output[i]:  #classification is incorrect\n",
    "        true_label = gold[i]\n",
    "        predicted_label = all_vader_output[i]\n",
    "\n",
    "        misclassified_counts[true_label] += 1\n",
    "\n",
    "        misclassified_tweets[true_label].append((tweets[i], predicted_label))\n",
    "\n",
    "# Summary of misclassified words\n",
    "print(\"\\nMisclassification Summary\")\n",
    "print(\"-\" * 50)\n",
    "for sentiment, count in misclassified_counts.items():\n",
    "    print(f\"{sentiment.capitalize()} misclassified: {count}\")\n",
    "print(\"*\" * 50)\n",
    "for sentiment, errors in misclassified_tweets.items():\n",
    "    print(f\"\\nMisclassified {sentiment.capitalize()} Tweets\")\n",
    "    print(\"~\" * 50)\n",
    "\n",
    "    for tweet, predicted in errors: # print the tweets\n",
    "        print(f\"\\nTweet: {tweet}\")\n",
    "        print(f\"Expected: {sentiment}; Predicted: {predicted}\")\n",
    "        \n",
    "        print(\"\\nVADER with verbose:\") # see what vader assigns\n",
    "        run_vader(tweet, lemmatize=True, verbose=1)\n",
    "        print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(B) Error Analysis**\n",
    "In total, 0 positive tweets were misclassified, 4 negative tweets were misclassified as neutral, and 4 neutral were misclassified as positive.\n",
    "\n",
    "1. Misclassified Positive Tweets\n",
    "* All true-positive tweets are correctly identified.\n",
    "\n",
    "2. Misclassified Negative Tweets\n",
    "* All 4 true-negative tweets were misclassified as neutral. In all of them, we see that the `neu` score is 1.0, meaning that the classifier is absolutely sure the tweets convey a neutral tone. Therefore, its lexicon fails to associate the negative words in the sentence with a negative sentiment. For example, the tweet `\"The traffic today was unbearable. Took me whole two hours to get home.\"` clearly carries a negative sentiment, because of the word `unbearable`, which seems to be missing from VARDER's lexicon, possibly accounting for the fact that it classified the tweet as `neutral`. Similarly, the words `cold` and `tasteless` from the second tweet `\"The food at the restaurant was cold and tasteless. Never going back.\"` are also absent from VADER's lexicon, which leads to a misclassification as neutral despite the negative sentiment. The words `slow` and `cold` (again) indicating a negative sentiment in the third tweet `\"The service at the cafe was slow and the coffee was cold.\"` are missing as well, contributing to its neutral classification. Lastly, the terms `overpriced` and `mediocre` from the fourth tweet `\"The restaurant was overpriced and the food was mediocre.\"` are not found in VADER's lexicon, which likely resulted in the neutral classification of the tweet, even though it expresses a negative tone.\n",
    "\n",
    "3. Misclassified Neutral Tweets\n",
    "* As shown in the performance analysis, the classifier has the lowest performance score when it comes to neutral tweets. All 4 of the misclassified neutral tweets were classified as positive, suggesting that VADER tends to assign a sentiment even when the intended tone is neutral. A common issue observed is that VADER overweights slightly positive words, such as `innovation`, `exploration`, `useful`, and `like`. These words do not necessarily indicate strong positive emotion, but VADER assigns them a positive sentiment score. For example, in For example, in `\"Reading about the latest tech innovations. There have been a lot of new developments.\"`, the word `innovations` is factual rather than an expression of enthusiasm, yet it leads to a positive classification. The same is true for the word `exploration` in `\"Listening to a podcast about space exploration. So much new information...\"`, which also causes a misclassification. Moreover, VADER lacks context awareness, causing misinterpretations of the tone. In `\"Would you like to watch the sunset at the park with me tonight?\"`, the phrase `would you like` is not an expression of excitement but rather a neutral request, yet VADER assigns it a positive connotation. Similarly, in `\"Listening to a podcast about productivity. It provides many useful tips.\"`, the word `useful` describes a fact rather than carrying a positive sentiment, yet it considers the tweet to be positive.\n",
    "* In addition to the points made above, another possible reason for the observed misclassifications is that the threshold for classifying a tweet as positive is too lenient. Currently, any `compound` score greater than `0.0` results in a positive classification, even if the score is barely a positive number. This means that tweets with weakly positive words, even if the intended tone is neutral, are pushed towards the positive category. Raising the threshold slightly could help reduce these errors and better differentiate between truly neutral and positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joanapetkova/Documents/VU AI/Text Mining/TextMiningGroup8/Lab3/airlinetweets\n"
     ]
    }
   ],
   "source": [
    "# path to folder\n",
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path().resolve()\n",
    "path_to_folder = Path.joinpath(cur_dir, r\"airlinetweets\")\n",
    "#path_to_folder = Path.joinpath(cur_dir, r\"airlinetweets/airlinetweets\")\n",
    "\n",
    "print(path_to_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@JetBlue great flight on a brand new jet. Great seating. Beautiful plane. Big fan of this airline.', 'positive')\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "import os\n",
    "\n",
    "data = []\n",
    "categories = [\"positive\", \"negative\", \"neutral\"]\n",
    "for c in categories:\n",
    "    category_path = os.path.join(path_to_folder, c)\n",
    "\n",
    "    for txt_file in os.listdir(category_path):\n",
    "        txt_file_path = os.path.join(category_path, txt_file)\n",
    "        \n",
    "        with open(txt_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            tweet = file.read().strip()\n",
    "            data.append((tweet, c))\n",
    "\n",
    "# checking random example\n",
    "print(data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Basic VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.64      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = set()\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: VADER with Lemmatized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: VADER with Only Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.20      0.33      1750\n",
      "     neutral       0.40      0.89      0.55      1515\n",
      "    positive       0.67      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.64      0.51      0.47      4755\n",
      "weighted avg       0.65      0.50      0.46      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = {\"ADJ\", \"JJ\", \"JJR\", \"JJS\"}  # include universal as well as Penn Treebank tags for adjective\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: VADER with Lemmatized Text of Only Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.20      0.33      1750\n",
      "     neutral       0.40      0.89      0.55      1515\n",
      "    positive       0.67      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.64      0.51      0.47      4755\n",
      "weighted avg       0.65      0.50      0.46      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = {\"ADJ\", \"JJ\", \"JJR\", \"JJS\"}  # include universal as well as Penn Treebank tags for adjective\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: VADER with Only Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.14      0.23      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.53      0.35      0.42      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.44      0.39      4755\n",
      "weighted avg       0.55      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = {\"NOUN\", \"NN\", \"NNS\"}  # include universal as well as Penn Treebank tags for nouns (proper nouns not included)\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: VADER with Lemmatized Text of Only Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.15      0.25      1750\n",
      "     neutral       0.36      0.81      0.50      1515\n",
      "    positive       0.52      0.34      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.44      0.39      4755\n",
      "weighted avg       0.54      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = {\"NOUN\", \"NN\", \"NNS\"}  # include universal as well as Penn Treebank tags for nouns (proper nouns not included)\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: VADER with Only Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.29      0.42      1750\n",
      "     neutral       0.38      0.81      0.52      1515\n",
      "    positive       0.57      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.46      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = {\"VERB\", \"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}  # include universal as well as Penn Treebank tags for verbs\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8: VADER with Lemmatized Text of Only Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.29      0.42      1750\n",
      "     neutral       0.38      0.78      0.51      1515\n",
      "    positive       0.57      0.36      0.44      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.57      0.48      0.46      4755\n",
      "weighted avg       0.58      0.47      0.46      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = {\"VERB\", \"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}  # include universal as well as Penn Treebank tags for verbs\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison**\n",
    "\n",
    " **Before discussing the results, we want to note that for balanced multi-class classification, the micro averages are equal to accuracy because both are computed using the total count of correct predictions divided by the total number of samples.**\n",
    "\n",
    "\n",
    "Looking at the results, we can observe that the basic VADER model from Experiment 1, which applies no lemmatization and considers all parts of speech, seems to perform the best, achieving the highest results in terms of accuracy (42%) and macro averages of precision (65%), recall (64%) and F1-scores (62%). The VADER model from Experiment 2, which applies lemmatization and also considers all parts of speech, follows with only a slight decrease of 1-2% in the accuracy and macro average metrics.\n",
    "\n",
    "When restricting input to specific parts of speech as in Experiments 3 to 8, a drop in the performance of VADER in terms of all average metrics is observed compared to the versions that consider all parts of speech. Adjectives alone (Experiments 3 and 4) yielded the best results compared to using only nouns or only verbs, with accuracy of 50%, macro average precision of 64%, average macro recall of 51%, and macro average F1-score of 47%. There was no difference in averages between versions with and without lemmatization that use only adjectives. Verbs alone (Experiments 7 and 8) were second best, with accuracy of 47%, macro average precision of 57-58%, macro average recall of 48%, and macro average F1-score of 46%. There was a slight difference in the average precision between the versions that use only verbs with and without lemmatization. Finally, using only nouns (Experiments 5 and 6) led to the worst results, with accuracy of 42%, macro average precision of 53-54%, macro average recall of 44%, and macro average F1-score of 39%. Again, a difference of 1% was observed in the average precision between the versions that use only nouns with and without lemmatization. Thus, the results indicate that adjectives contribute the most sentiment to sentiment classification followed by verbs, which aligns with the fact that sentiment is often conveyed through descriptive words (e.g.\"pleasant\", \"exploit\").\n",
    "\n",
    "All versions of VADER do not exhibit significant differences between the accuracy, macro averages of recall and F1-score (the maximum difference is 3 %). However, most of them (except the versions from Experiments 1 and 2, which look at all parts of speech) exhibit a difference of around 10% between micro average precision (accuracy) and macro average precision, with the micro average (accuracy) being lower. Since the proportions of the different sentiment categories are relatively balanced, this difference is most likely due to VADER struggling more with certain sentiment categories, particularly neutral sentiment, which has lower precision than positive and negative sentiment in all experiments with POS filtering. Since micro averages consider the number of examples from each category, poor performance on one category (e.g., neutral) pulls down micro average precision (accuracy) more than macro average precision, which treats all categories equally.\n",
    "\n",
    "Finally, the trend between sentiment categories (negative, neutral, positive) overall remains consistent across different experiments, though the exact values fluctuate depending on whether lemmatization or part-of-speech filtering is applied. In most experiments, the negative sentiment has high precision but low recall, meaning that when VADER predicts a tweet as negative, it is usually correct, but it often fails to detect all negative tweets. The precision of the neutral sentiment is lower than the recall for experiments that apply POS filtering, while the opposite is observed for those that do not apply it. This means that in the cases where certain parts of speech are filtered, VADER frequently classifies tweets as neutral when many of those tweets should have been classified as positive or negative instead. Likely due to the filtering, there is not sufficient information about the sentiment of those tweets, so VADER defaults to neutral classification. In contrast to neutral sentiment, the precision of the positive sentiment is higher than the recall for experiments that apply POS filtering, while the opposite is observed for those that do not apply it. In the cases without filtering, VADER captures many positive tweets correctly, but in the process, it also misclassifies some neutral or negative tweets as positive.\n",
    "\n",
    "The F1-score for all sentiments also remains relatively stable across experiments, being considerably lower for versions that filter parts of speech compared to those that do not. In the versions without filtering, the F1 score for positive sentiment is highest, followed by the F1-score for negative sentiment, and then the F1-score for neutral sentiment. On the other hand, when POS filtering is applied, the F1 score for neutral sentiment is highest, followed by the F1-score for positive sentiment, and then the F1-score for negative sentiment. Based on these observations, it seems that VADER is slightly better at detecting positivity than negativity.\n",
    "\n",
    "#### **Does lemmatization help?**\n",
    "\n",
    "Based on the fact that almost no difference in results is seen between experiments with and without lemmatization that use the same parts of speech (Experiments 1 vs. 2, 3 vs. 4, 5 vs. 6, and 7 vs. 8,) as described in detail above, we can conclude that lemmatization does not significantly improve sentiment classification. The accuracy and macro average scores remain nearly identical with at most a 2% difference, usually in favor of the non-lemmatized text. This suggests that sentiment classification with VADER is not highly dependent on whether words are in their base form. One possible reason is that VADER's sentiment lexicon already accounts for many common word variations, so reducing words to their lemma does not add much value.\n",
    "\n",
    "#### **Are all parts of speech equally important for sentiment analysis?**\n",
    "\n",
    "No, all parts of speech are not equally important for sentiment analysis. The results show that using only adjectives leads to the best performance, followed by using only verbs, and finally using only nouns yields the worst classification, as discussed in detail above. This makes sense because adjectives are more directly associated with sentiment (e.g., \"amazing\", \"annoying\", \"disappointing\"), and some verbs also express certain emotions (e.g., \"appreciate\", \"struggle\", \"reject\"), whereas nouns often carry only a small amount of direct emotional weight.\n",
    "\n",
    "Overall, VADER exhibits the best performance when the full text of a tweet is used. Removing any part of speech reduces the classification scores. This suggests that while adjectives and verbs seem to be the most important, a combination of all parts of speech contributes to a better sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "airline_tweets = load_files(\n",
    "    \"airlinetweets\",\n",
    "    categories=[\"neutral\", \"positive\", \"negative\"],\n",
    "    encoding=\"utf-8\",\n",
    "    shuffle=True,\n",
    "    random_state=0)\n",
    "\n",
    "tweets = airline_tweets.data\n",
    "labels = airline_tweets.target\n",
    "label_names = airline_tweets.target_names\n",
    "labels = [label_names[label] for label in labels]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, train_size=0.8, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic settings (TF-IDF representation, min_df=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.91      0.84       336\n",
      "     neutral       0.85      0.65      0.74       303\n",
      "    positive       0.84      0.88      0.86       312\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.81       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training with default settings (TF-IDF representation, min_df=2)\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n",
    "\n",
    "# using Multinomial NB because it is good for txt classification\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorizer, y_train)\n",
    "y_pred = model.predict(X_test_vectorizer)\n",
    "\n",
    "print(\"Basic settings (TF-IDF representation, min_df=2)\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words representation ('airline_count')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.90      0.86       336\n",
      "     neutral       0.86      0.70      0.77       303\n",
      "    positive       0.83      0.88      0.85       312\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIFFERENT SETTINGS 1\n",
    "# with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count')\n",
    "\n",
    "# Bag of words representation ('airline_count')\n",
    "# for BoW use CountVectorizer\n",
    "vectorizer_count = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)  \n",
    "X_train_counter = vectorizer_count.fit_transform(X_train)\n",
    "X_test_counter = vectorizer_count.transform(X_test)\n",
    "\n",
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_counter, y_train)\n",
    "y_pred_count = model_count.predict(X_test_counter)\n",
    "\n",
    "print(\"Bag of words representation ('airline_count')\")\n",
    "print(classification_report(y_test, y_pred_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vs BoW\n",
    "\n",
    "To compare TF-IDF and BoW results, we analyze the output of the classification_report. First of all, the overall accuracy is slightly better with BoW than with TF-IDF (83% and 82%, respectively). Furthermore, if we compare by category, the results are very similar for the neutral and positive categories (TF-IDF has slightly better precision, but the difference is very small). \n",
    "\n",
    "Neutral category: \n",
    "- TF-IDF: Precision 0.85, Recall 0.65, F1-score 0.74; \n",
    "- BoW: Precision 0.86, Recall 0.70, F1-score 0.77\n",
    "\n",
    "--> BoW has slighly better precision but the difference is very small\n",
    "\n",
    "Positive category:\n",
    "- TF-IDF: Precision 0.84, Recall 0.88, F1-score 0.86;\n",
    "- BoW: Precision 0.83, Recall 0.88, F1-score 0.85\n",
    "\n",
    "--> TF-IDF has slighly better precision but the difference is very small\n",
    "\n",
    "Negative category:\n",
    "- TF-IDF: Precision 0.79, Recall 0.91, F1-score 0.84;\n",
    "- BoW: Precision 0.82, Recall 0.90, F1-score 0.86\n",
    "\n",
    "--> for the negative category, BoW performs better- it results in higher precision (0.82 vs. 0.79) while maintaing very similar recall (0.91 for TF-IDF and 0.90 for BoW). From this, we can conclude that BoW produces fewer false positives. The overall higher accuracy of BoW can be explained by its better performance on negative tweets.\n",
    "\n",
    "\n",
    "These differences are due to the specificity of both text vectorization techniques:\n",
    "1) BoW counts how many times each word appears in the tweet (ignoring grammar, order, meaning). It is a simpler approach and can be effective for capturing common words. However, it does not consider the importance of words, meaning that even frequent but unimportant words (such as \"the\") can receive high weight.\n",
    "2) TF-IDF is similar to BoW but it adjusts for word importance. Instead of just taking raw word counts, it gives weight to words that are more important in a document. It assigns weights based on term frequency (TF) and inverse document frequency (IDF), where IDF penalizes common words, reducing their influence in classification.\n",
    "\n",
    "In the case of our tweets, BoW slightly outperformed TF-IDF because tweets contain strong, frequent words that aid classification. Examples of frequent negative words include \"bad\", \"worst\", \"delayed\", \"cancelled\", \"rude\", \"late\", \"horrible\", and \"never\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with TF-IDF, min_df=2\n",
      "TF-IDF classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.91      0.84       336\n",
      "     neutral       0.85      0.65      0.74       303\n",
      "    positive       0.84      0.88      0.86       312\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.81       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n",
      "\n",
      "Training with BoW, min_df=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.90      0.86       336\n",
      "     neutral       0.86      0.70      0.77       303\n",
      "    positive       0.83      0.88      0.85       312\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n",
      "\n",
      "Training with TF-IDF, min_df=5\n",
      "TF-IDF classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.88      0.84       336\n",
      "     neutral       0.81      0.70      0.75       303\n",
      "    positive       0.83      0.86      0.84       312\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.82      0.81      0.81       951\n",
      "weighted avg       0.82      0.81      0.81       951\n",
      "\n",
      "\n",
      "Training with BoW, min_df=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.89      0.85       336\n",
      "     neutral       0.85      0.73      0.79       303\n",
      "    positive       0.84      0.88      0.86       312\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      "\n",
      "\n",
      "Training with TF-IDF, min_df=10\n",
      "TF-IDF classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.87      0.83       336\n",
      "     neutral       0.78      0.69      0.73       303\n",
      "    positive       0.82      0.82      0.82       312\n",
      "\n",
      "    accuracy                           0.80       951\n",
      "   macro avg       0.80      0.79      0.79       951\n",
      "weighted avg       0.80      0.80      0.80       951\n",
      "\n",
      "\n",
      "Training with BoW, min_df=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.88      0.85       336\n",
      "     neutral       0.83      0.73      0.78       303\n",
      "    positive       0.85      0.88      0.86       312\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIFFERENT SETTINGS 2\n",
    "# with respect to the frequency threshold (min_df)\n",
    "\n",
    "# values of document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "min_df_values = [2, 5, 10]\n",
    "\n",
    "# train a model with respect to each frequency threshold\n",
    "for min_df in min_df_values:\n",
    "    print(f\"\\nTraining with TF-IDF, min_df={min_df}\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=min_df)\n",
    "    X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorizer = vectorizer.transform(X_test)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vectorizer, y_train)\n",
    "    y_pred = model.predict(X_test_vectorizer)\n",
    "\n",
    "    print(\"TF-IDF classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nTraining with BoW, min_df={min_df}\")\n",
    "    \n",
    "    vectorizer_bow = CountVectorizer(min_df=min_df, tokenizer=nltk.word_tokenize)\n",
    "    X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "    X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "    model_bow = MultinomialNB()\n",
    "    model_bow.fit(X_train_bow, y_train)\n",
    "    y_pred_bow = model_bow.predict(X_test_bow)\n",
    "\n",
    "    print(\"BoW classification report:\")\n",
    "    print(classification_report(y_test, y_pred_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency threshold experiment results\n",
    "\n",
    "#### Classification report results\n",
    "\n",
    "1) TF-IDF vs. BoW (min_df=2)\n",
    "\n",
    "As mentioned in the result analysis in the previous section, the accuracy for BoW is slightly better at 83% compared to TF-IDF’s 82%. \n",
    "\n",
    "Per category:\n",
    "- Negative tweets: BoW outperforms TF-IDF with higher precision (0.82 vs 0.79) --> BoW has fewer false positives\n",
    "- Neutral tweets: TF-IDF has slightly lower precision (0.85 vs 0.86) and lower recall (0.65 vs 0.70) but the difference is also small (TF-IDF model is missing more relevant instances)\n",
    "- Positive tweets: TF-IDF has slightly better precision (0.84 vs 0.83) but the recall is the same (0.88) \n",
    "\n",
    "As pointed out in the previous comparison, BoW slightly outperforms TF-IDF due to better performance on negative tweets (see the details above).\n",
    "\n",
    "2) TF-IDF vs. BoW (min_df=5)\n",
    "\n",
    "The overall accuracy slightly changed. TF-IDF has 81%, while BoW scores 83% --> it is still slightly better for BoW.\n",
    "\n",
    "Per category:\n",
    "- Negative tweets: BoW outperforms TF-IDF with higher precision (0.82 vs 0.79) and recall is lower for BoW (0.88 vs 0.89)\n",
    "- Neutral tweets: Results are very close. BoW performs slightly better with precision (0.81 vs 0.85) and also the recall is higher (0.70 vs 0.73)\n",
    "- Positive tweets: Results are very close. BoW has slightly better recall (0.88 vs 0.86), the precision is slightly better in BoW too (0.84 vs 0.83)\n",
    "\n",
    "In this case, both TF-IDF and BoW resulted in similar accuracy as before. BoW is again better at handling negative tweets but also this time better at classifying neutral tweets. The performance is closer for the positive categories. Increasing min_df from 2 to 5 does not significantly impact the overall accuracy but slightly improves BoW’s performance in neutral tweets.\n",
    "\n",
    "3) TF-IDF vs. BoW (min_df=10)\n",
    "\n",
    "BoW achieves 83% accuracy, while TF-IDF results in 80% accuracy. Compared to previous experiments, the accuracy of BoW remains the same, while the accuracy of TF-IDF has decreased by 1%.\n",
    "\n",
    "Per category:\n",
    "- Negative tweets: BoW has higher precision (0.81 vs. 0.79) and higher recall (0.88 vs. 0.87)\n",
    "- Neutral tweets: TF-IDF has slightly better recall (0.69 vs. 0.73), but BoW has better precision (0.83 vs. 0.78)\n",
    "- Positive tweets: TF-IDF has slightly lower recall (0.82 vs. 0.88) and slightly better precision (0.82 vs. 0.85)\n",
    "\n",
    "In this case, BoW achieves 83% accuracy, while TF-IDF drops to 80% accuracy. Compared to min_df=2 and min_df=5, BoW remains stable but TF-IDF’s accuracy has decreased. BoW continues to outperform TF-IDF in negative tweets, maintaining higher precision and recall . For neutral tweets, BoW improves in precision, while TF-IDF’s recall drops further, indicating that TF-IDF struggles more with capturing neutral tweets as min_df increases. In positive tweets, BoW achieves higher recall, while TF-IDF retains slightly better precisions.\n",
    "\n",
    "#### Impact of increasing min_df\n",
    "\n",
    "Increasing the min_df from 2 to 10 removes rare words, resulting in a smaller vocabulary size. BoW is more affected by this change because it relies on raw word counts, whereas TF-IDF adjusts more easily by assigning weights to important terms. As a result, the accuracy of both models remains similar despite the reduction in vocabulary size. In conclusion, eliminating rare words has minimal impact on classification performance, suggesting that the classification of these tweets depends more on frequently occurring words rather than rare ones. The sentiment of airline tweets is primarily influenced by common terms, and even with min_df=10, the model maintains consistent accuracy. This indicates that the key words driving sentiment classification are shared across many tweets. Even with min_df = 10, the model maintains a similar accuracy --> `The key words influencing sentiment classification are common across many tweets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In conclusion, we saw different text vectorization techniques and their impact on classification performance. The experiments compared TF-IDF technique and Bag of Words technique, which demonstrated that while both methods yield similar results, BoW slightly outperformed TF-IDF in negative tweet classification. This is likely because negative sentiment is often expressed by using strong, frequently occuring words. For this, BoW is effective and captures frequent words without weighting adjustments. Moreover, the impact of increasing the frequency treshold (min_df) was explored. Removing more rare words (increasing min_df from 2 to 5 and then to 10) had minimal impact on classification performance. Therefore, sentiment in airline tweets is mostly determined by frequently used words. Overall, depending on the dataset and task, it is important to choose the right text representation–TF-IDF is useful for taking into account word importance; BoW is strong for fields where sentiment words occur frequently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "negative 1518.0 @\n",
      "negative 1390.0 united\n",
      "negative 1230.0 .\n",
      "negative 779.0 to\n",
      "negative 590.0 i\n",
      "negative 527.0 the\n",
      "negative 448.0 a\n",
      "negative 426.0 ``\n",
      "negative 405.0 flight\n",
      "negative 371.0 ?\n",
      "negative 368.0 !\n",
      "negative 365.0 you\n",
      "negative 356.0 and\n",
      "negative 337.0 for\n",
      "negative 327.0 my\n",
      "negative 325.0 on\n",
      "negative 324.0 #\n",
      "negative 297.0 is\n",
      "negative 280.0 in\n",
      "negative 228.0 n't\n",
      "negative 211.0 of\n",
      "negative 208.0 your\n",
      "negative 198.0 that\n",
      "negative 190.0 it\n",
      "negative 176.0 not\n",
      "negative 173.0 me\n",
      "negative 166.0 have\n",
      "negative 164.0 was\n",
      "negative 163.0 ''\n",
      "negative 159.0 with\n",
      "negative 151.0 no\n",
      "negative 145.0 at\n",
      "negative 126.0 's\n",
      "negative 124.0 this\n",
      "negative 112.0 service\n",
      "negative 109.0 from\n",
      "negative 109.0 be\n",
      "negative 108.0 do\n",
      "negative 107.0 virginamerica\n",
      "negative 103.0 we\n",
      "negative 100.0 cancelled\n",
      "negative 99.0 an\n",
      "negative 98.0 now\n",
      "negative 98.0 get\n",
      "negative 94.0 :\n",
      "negative 91.0 why\n",
      "negative 90.0 but\n",
      "negative 89.0 delayed\n",
      "negative 89.0 customer\n",
      "negative 88.0 plane\n",
      "negative 88.0 are\n",
      "negative 85.0 time\n",
      "negative 85.0 just\n",
      "negative 84.0 they\n",
      "negative 83.0 bag\n",
      "negative 82.0 been\n",
      "negative 77.0 ;\n",
      "negative 77.0 'm\n",
      "negative 75.0 ...\n",
      "negative 74.0 hours\n",
      "negative 73.0 so\n",
      "negative 73.0 -\n",
      "negative 72.0 &\n",
      "negative 67.0 what\n",
      "negative 67.0 gate\n",
      "negative 67.0 can\n",
      "negative 64.0 will\n",
      "negative 62.0 would\n",
      "negative 62.0 still\n",
      "negative 62.0 amp\n",
      "negative 61.0 when\n",
      "negative 61.0 late\n",
      "negative 61.0 how\n",
      "negative 60.0 our\n",
      "negative 60.0 has\n",
      "negative 60.0 did\n",
      "negative 60.0 about\n",
      "negative 59.0 http\n",
      "negative 59.0 hour\n",
      "negative 59.0 again\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "neutral 1397.0 @\n",
      "neutral 587.0 to\n",
      "neutral 525.0 ?\n",
      "neutral 525.0 .\n",
      "neutral 455.0 i\n",
      "neutral 311.0 the\n",
      "neutral 306.0 jetblue\n",
      "neutral 291.0 :\n",
      "neutral 275.0 southwestair\n",
      "neutral 269.0 a\n",
      "neutral 262.0 you\n",
      "neutral 249.0 united\n",
      "neutral 242.0 on\n",
      "neutral 238.0 ``\n",
      "neutral 235.0 #\n",
      "neutral 228.0 flight\n",
      "neutral 202.0 for\n",
      "neutral 196.0 my\n",
      "neutral 195.0 americanair\n",
      "neutral 183.0 http\n",
      "neutral 182.0 in\n",
      "neutral 171.0 !\n",
      "neutral 167.0 is\n",
      "neutral 160.0 usairways\n",
      "neutral 157.0 can\n",
      "neutral 142.0 and\n",
      "neutral 136.0 's\n",
      "neutral 130.0 from\n",
      "neutral 125.0 of\n",
      "neutral 123.0 it\n",
      "neutral 116.0 do\n",
      "neutral 113.0 me\n",
      "neutral 112.0 have\n",
      "neutral 91.0 with\n",
      "neutral 76.0 this\n",
      "neutral 76.0 any\n",
      "neutral 75.0 that\n",
      "neutral 75.0 at\n",
      "neutral 73.0 get\n",
      "neutral 73.0 be\n",
      "neutral 72.0 what\n",
      "neutral 72.0 -\n",
      "neutral 70.0 please\n",
      "neutral 70.0 if\n",
      "neutral 70.0 are\n",
      "neutral 69.0 will\n",
      "neutral 68.0 virginamerica\n",
      "neutral 67.0 flights\n",
      "neutral 64.0 )\n",
      "neutral 62.0 we\n",
      "neutral 62.0 help\n",
      "neutral 60.0 our\n",
      "neutral 59.0 need\n",
      "neutral 58.0 there\n",
      "neutral 58.0 ''\n",
      "neutral 56.0 (\n",
      "neutral 55.0 n't\n",
      "neutral 55.0 just\n",
      "neutral 54.0 your\n",
      "neutral 51.0 ;\n",
      "neutral 49.0 how\n",
      "neutral 46.0 when\n",
      "neutral 46.0 or\n",
      "neutral 44.0 out\n",
      "neutral 43.0 dm\n",
      "neutral 43.0 &\n",
      "neutral 42.0 us\n",
      "neutral 42.0 so\n",
      "neutral 42.0 not\n",
      "neutral 41.0 tomorrow\n",
      "neutral 40.0 an\n",
      "neutral 40.0 ...\n",
      "neutral 38.0 “\n",
      "neutral 38.0 now\n",
      "neutral 38.0 know\n",
      "neutral 38.0 fleet\n",
      "neutral 38.0 fleek\n",
      "neutral 37.0 would\n",
      "neutral 36.0 ”\n",
      "neutral 36.0 thanks\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "positive 1328.0 @\n",
      "positive 1008.0 !\n",
      "positive 748.0 .\n",
      "positive 456.0 you\n",
      "positive 426.0 to\n",
      "positive 423.0 the\n",
      "positive 343.0 for\n",
      "positive 317.0 #\n",
      "positive 306.0 southwestair\n",
      "positive 295.0 i\n",
      "positive 284.0 jetblue\n",
      "positive 277.0 thanks\n",
      "positive 259.0 united\n",
      "positive 251.0 thank\n",
      "positive 238.0 ``\n",
      "positive 215.0 a\n",
      "positive 195.0 and\n",
      "positive 180.0 flight\n",
      "positive 168.0 americanair\n",
      "positive 167.0 :\n",
      "positive 157.0 my\n",
      "positive 156.0 on\n",
      "positive 132.0 in\n",
      "positive 130.0 great\n",
      "positive 129.0 usairways\n",
      "positive 113.0 it\n",
      "positive 113.0 is\n",
      "positive 109.0 your\n",
      "positive 109.0 of\n",
      "positive 107.0 me\n",
      "positive 105.0 so\n",
      "positive 98.0 with\n",
      "positive 90.0 was\n",
      "positive 90.0 )\n",
      "positive 88.0 service\n",
      "positive 88.0 at\n",
      "positive 83.0 this\n",
      "positive 82.0 virginamerica\n",
      "positive 70.0 http\n",
      "positive 67.0 just\n",
      "positive 67.0 best\n",
      "positive 67.0 are\n",
      "positive 66.0 love\n",
      "positive 63.0 much\n",
      "positive 63.0 from\n",
      "positive 63.0 ;\n",
      "positive 62.0 that\n",
      "positive 61.0 's\n",
      "positive 60.0 guys\n",
      "positive 59.0 customer\n",
      "positive 58.0 have\n",
      "positive 55.0 awesome\n",
      "positive 52.0 -\n",
      "positive 50.0 we\n",
      "positive 50.0 airline\n",
      "positive 49.0 good\n",
      "positive 49.0 be\n",
      "positive 47.0 time\n",
      "positive 45.0 up\n",
      "positive 45.0 out\n",
      "positive 45.0 all\n",
      "positive 44.0 amazing\n",
      "positive 42.0 &\n",
      "positive 40.0 us\n",
      "positive 40.0 got\n",
      "positive 38.0 today\n",
      "positive 38.0 crew\n",
      "positive 36.0 fly\n",
      "positive 35.0 will\n",
      "positive 35.0 they\n",
      "positive 35.0 our\n",
      "positive 35.0 not\n",
      "positive 35.0 n't\n",
      "positive 35.0 flying\n",
      "positive 35.0 amp\n",
      "positive 34.0 do\n",
      "positive 34.0 ...\n",
      "positive 33.0 now\n",
      "positive 33.0 made\n",
      "positive 31.0 very\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out() # had to change this line of code to _out(), because in newer version of scikit-learn it was removed\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# x_train = [\" \".join(word_tokenize(text)) for text in x_train]  # we can use this if we want to remove punctuation and maybe improve our model\n",
    "# x_test = [\" \".join(word_tokenize(text)) for text in x_test]\n",
    "\n",
    "# Reuse train-test split from previous question\n",
    "vectorizer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize) # remove tokenizer argument if we want to remove punctuation, and uncomment the lines above\n",
    "train_counts = vectorizer.fit_transform(X_train)\n",
    "test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, y_train)\n",
    "\n",
    "\n",
    "important_features_per_class(vectorizer, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Features\n",
    "\n",
    "#### Negative Class\n",
    "**Expected Words:** `\"delayed\"`, `\"cancelled\"`, `\"late\"`\n",
    "\n",
    "**Why?** People usually express frustration on Twitter when their flight is delayed, canceled, or when they experience poor service.\n",
    "\n",
    "#### Neutral Class\n",
    "**Expected Words:** `\"flight\"`, `\"gate\"`\n",
    "\n",
    "**Why?** Neutral tweets are often just factual, mentioning general travel related words without expressing strong emotions, this also makes it harder to predict neutral words.\n",
    "\n",
    "#### Positive Class\n",
    "**Expected Words:**  `\"best\"`, `\"great\"`, `\"amazing\"`    \n",
    "\n",
    "**Why?** Happy customers tend to express gratitude and enthusiasm when they receive good service.\n",
    "\n",
    "---\n",
    "\n",
    "### Unexpected Features \n",
    "Some unexpected features appeared in the output, such as punctuation and symbols like `\"@\"`, `\".\"`, `\"!\"`, `\"?\"`, and `\"#\"`, which are structural elements of tweets but do not carry sentiment. Additionally, common stopwords such as `\"the\"`, `\"a\"`, `\"is\"`, `\"with\"`, and `\"that\"` were surprisingly ranked high despite being function words that frequently appear in all tweets without having meaningful sentiment. Furthermore, numbers and generic words like `\"hours\"`and `\"http\"` were unexpected. While `\"hour\"` and `\"hours\"` may sometimes indicate delays, numerical values and URLs generally do not convey sentiment on their own.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Words That Should Be Kept Or Removed\n",
    "To improve the classification model, we should remove words that don’t add sentiment meaning and keep those that do.\n",
    "\n",
    "#### Words to Remove that have No Sentiment Meaning\n",
    "- **Punctuation**: `\".\"`, `\"!\"`, `\"?\"`, `\"...\"`, `\"@\"`, `\"#\"`  \n",
    "- **Stopwords**: `\"the\"`, `\"to\"`, `\"a\"`, `\"is\"`, `\"in\"`, `\"with\"`, `\"that\"`, `\"be\"`  \n",
    "- **Mentions & URLs**: `\"@\"`, `\"http\"`  \n",
    "- **Symbols**: `\"&\"`, `\"-\"`, `\":\"`, `\"''\"`\n",
    "\n",
    "#### Words to Keep that have Strong Sentiment Meaning\n",
    "- **Negative Sentiment:** `\"cancelled\"`, `\"delayed\"` \n",
    "- **Neutral Words:** `\"service\"`,`\"flight\"`, `\"gate\"`  \n",
    "- **Positive Sentiment:** `\"best\"`, `\"great\"`,`\"good\"`, `\"awesome\"`, `\"amazing\"`  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Model Improvement \n",
    "To make the classification more accurate, we can apply the following techniques:\n",
    "\n",
    "1. **Remove Stopwords**: Use `stop_words=\"english\"` in `CountVectorizer` to filter out common words that don’t add meaning.  \n",
    "2. **Exclude Punctuation** \n",
    "4. **Apply Stemming or Lemmatization**: Convert words to their root form (e.g., `\"delayed\"` → `\"delay\"`, `\"cancelled\"` → `\"cancel\"`).  \n",
    "\n",
    "By applying these refinements, we can improve our Naive Bayes model's ability to classify tweets more accurately and meaningfully.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
