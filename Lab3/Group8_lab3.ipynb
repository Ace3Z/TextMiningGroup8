{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence 1: \"I love apples\"\n",
    "\n",
    "**Explanation:**  \n",
    "In this sentence, VADER identifies the word \"love\" as strongly positive. The remaining words (\"I\" and \"apples\") are not associated with any sentiment in the lexicon. So, the overall sentiment is driven almost completely by \"love,\" resulting in a high positive score.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 2: \"I don't love apples\"\n",
    "\n",
    "**Explanation:**  \n",
    "The presence of the negation \"don't\" inverts the sentiment of the word \"love.\" VADER’s negation rule switches the polarity of a positive term to negative, leading to an overall negative sentiment for the sentence. This shows how even a single negator can significantly change the sentiment outcome.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 3: \"I love apples :-)\"\n",
    "\n",
    "**Explanation:**  \n",
    "The positive term \"love\" is again present, and the additional emoticon \":-)\" provides an extra boost. VADER is designed to recognize common emoticons and assign them sentiment values. The combination results in an even higher overall positive sentiment than the sentence without the emoticon.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 4: \"These houses are ruins\"\n",
    "\n",
    "**Explanation:**  \n",
    "The key term in this sentence is \"ruins,\" which in VADER’s lexicon carries a negative connotation. As a result, the sentence is interpreted as negative. However, the context is unknown, “ruins” could describe old structures as well but VADER does not perform context disambiguation, so it relies solely on the lexicon value.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 5: \"These houses are certainly not considered ruins\"\n",
    "\n",
    "**Explanation:**  \n",
    "In this sentence, the negative word \"ruins\" is modified by the negation \"not,\" which causes VADER to invert its sentiment. Additionally, the modifier \"certainly\" slightly improves the sentiment intensity. The inversion of a negative term leads VADER to interpret the sentence as leaning towards a positive sentiment, even though one might expect it to be more neutral.\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 6: \"He lies in the chair in the garden\"\n",
    "\n",
    "**Explanation:**  \n",
    "The word \"lies\" is ambiguous. Although it can mean \"reclines\" in a neutral sense, VADER’s lexicon associates \"lies\" with dishonesty, a negative trait. Without the ability to disambiguate between the meanings, VADER assigns a negative sentiment to the sentence, even though the intended meaning is simply descriptive (and, thus, one would be expect the sentiment to be neutral).\n",
    "\n",
    "\n",
    "\n",
    "## Sentence 7: \"This house is like any house\"\n",
    "\n",
    "**Explanation:**  \n",
    "In this case, the word \"like\" is interpreted by VADER as a positive signal. However, the sentence is meant to express a neutral comparison, that the house is unremarkable. Because VADER registers \"like\" with a positive bias, it results in a slight positive sentiment, which does not fully capture the neutral intent of the sentence.\n",
    "\n",
    "\n",
    "\n",
    "## **Final Outcome:**  \n",
    "VADER’s approach relies on a fixed lexicon and a set of heuristic rules. While it effectively captures clear positive and negative signals (handles negation and emoticons), its inability to disambiguate word meanings or interpret subtle contextual cues can lead to sentiment scores that do not always match the intended sentiment of the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "########################### imports below have been added manually for solving the questions in this NB\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'sentiment_label': 'neutral', 'text_of_tweet': 'Scientists discovered a new type of organism in the depths of the ocean.', 'tweet_url': 'manually created'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in list(my_tweets.items())[::-1]:\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **NB!** I don't exactly understand whether by \"explain which scores are most relevant and why\" they refer to the scores that VADER produces (neg, pos, neu, compound), or the scores from the classification report (f1, P, R, accuracy, etc.). Therefore, I explain both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse `run_vader` from ***Lab3.2*** *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', textual_unit) # change to textual_unit so the whole tweet is displayed\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        16\n",
      "     neutral       0.75      0.75      0.75        16\n",
      "    positive       0.82      1.00      0.90        18\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.83      0.84        50\n",
      "weighted avg       0.85      0.84      0.84        50\n",
      "\n",
      "   micro avg       0.84      0.84      0.84        50\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(gold, all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=gold, y_pred=all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=gold, y_pred=all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=gold, y_pred=all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11}{recall_micro:>10}{f1_score_micro:>10}{len(tweets):>10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(A) Quantitative Evaluation**\n",
    "VADER analyzes the polarity of words and produces 4 different sentiment scores for each text input:\n",
    "* Positive (`pos`): The proportion of words that express a positive sentiment\n",
    "* Negative (`neg`): The proportion of words that express a negative sentiment\n",
    "* Neutral (`neu`): The proportion of words that are neutral / lack clear sentiment\n",
    "* Compound (`compound`): An aggregated sentiment score; ranges from -1 (extremely negative) to +1 (extremely positive)\n",
    "\n",
    "After producing those 4 scores, the final sentiment label of the tweet is assigned based on the `compound` score, as detailed in the `vader_output_to_label` function. Therefore, the `compound` score is the most relevant score for the final label classification, which is done according to the following scheme:\n",
    "* **`compound` > 0** → *positive* label\n",
    "* **`compound` < 0** → *negative* label\n",
    "* **`compound` = 0** → *neutral* label\n",
    "\n",
    "In order to evaluate the performence of the VADER classifier, we used the scikit-learn's classification report. We present and explain the insights it provides:\n",
    "1. **Precision** (how many tweets did VADER classify correctly out of all classifications for a certain category):\n",
    "* Negative (`1.00`) - VADER achieves perfect precision on negative examples (correctly predicts them 100% of the time)\n",
    "* Neutral (`0.75`) - some tweets misclassified as neutral were actually positive or negative\n",
    "* Positive (`0.82`) - overall high precision on positive tweets, however, *18%* of those classified as positive are being misclassified\n",
    "2. **Recall** (how many tweets did VADER classify correctly out of all the tweets in a category (as determined by the gold label)):\n",
    "* Negative (`0.75`) - some negative tweets were misclassified as neutral or positive\n",
    "* Neutral (`0.75`) - VADER manages to identify *75%* of actual neutral tweets\n",
    "* Positive (`1.00`) - all actual positive tweets were detected correctly\n",
    "3. **F1-score** (harmonic mean of precision and recall):\n",
    "* Negative (`0.86`) - strong performance in detecting negative tweets\n",
    "* Neutral (`0.75`) - decent performance in classifying neutral tweets. Across the three categories, VADER has the lowest score for neutral tweets\n",
    "* Positive (`0.90`) - highly effective at identifying positive tweets\n",
    "4. **Accuracy** (overall percentage of correct classifications):\n",
    "* `84%` - VADER correctly classified *84%* of all tweets\n",
    "5. **Macro average**, **Weighted average** & **Micro average**           \n",
    "* Macro average - the average of precision, recall, and F1-score across all classes, treating each class equally (no weights)\n",
    "* Weighted average - the average of precision, recall, and F1-score, weighted by the number of samples in each class\n",
    "* Micro average - calculates precision, recall, and F1-score by aggregating the total number of correct and incorrect predictions across all classes (takes the sum of the raw values of true positives, false positives, and false negatives from all classes). Here, the micro average precision, recall, and F1-score are all 0.84\n",
    "* Micro, macro and weighted averages all showed nearly identical scores, suggesting that there are no severe class imbalance issues, and the model has a good stable performance across all classes\n",
    "\n",
    "For evaluating the overall performance, F1-scores and accuracy are most relevant as they give a good general idea of how well the classifier works. Accuracy alone can be a misleading metric, as it does not give insight on the types of errors the model makes and is sensitive to class imbalance - when one category is much more frequent that the rest (this is not the case here), therefore, combining it with F1-score, we can get a good overview of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Summary\n",
      "--------------------------------------------------\n",
      "Positive misclassified: 0\n",
      "Negative misclassified: 4\n",
      "Neutral misclassified: 4\n",
      "**************************************************\n",
      "\n",
      "Misclassified Positive Tweets\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Misclassified Negative Tweets\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Tweet: The traffic today was unbearable. Took me whole two hours to get home.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The traffic today was unbearable. Took me whole two hours to get home.\n",
      "INPUT TO VADER ['the', 'traffic', 'today', 'be', 'unbearable', '.', 'take', 'I', 'whole', 'two', 'hour', 'to', 'get', 'home', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Tweet: The food at the restaurant was cold and tasteless. Never going back.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The food at the restaurant was cold and tasteless. Never going back.\n",
      "INPUT TO VADER ['the', 'food', 'at', 'the', 'restaurant', 'be', 'cold', 'and', 'tasteless', '.', 'never', 'go', 'back', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Tweet: The service at the cafe was slow and the coffee was cold.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The service at the cafe was slow and the coffee was cold.\n",
      "INPUT TO VADER ['the', 'service', 'at', 'the', 'cafe', 'be', 'slow', 'and', 'the', 'coffee', 'be', 'cold', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Tweet: The restaurant was overpriced and the food was mediocre.\n",
      "Expected: negative; Predicted: neutral\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE The restaurant was overpriced and the food was mediocre.\n",
      "INPUT TO VADER ['the', 'restaurant', 'be', 'overprice', 'and', 'the', 'food', 'be', 'mediocre', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "**************************************************\n",
      "\n",
      "Misclassified Neutral Tweets\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Tweet: Reading about the latest tech innovations. There have been a lot of new developments.\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Reading about the latest tech innovations. There have been a lot of new developments.\n",
      "INPUT TO VADER ['read', 'about', 'the', 'late', 'tech', 'innovation', '.', 'there', 'have', 'be', 'a', 'lot', 'of', 'new', 'development', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'compound': 0.3818}\n",
      "**************************************************\n",
      "\n",
      "Tweet: Listening to a podcast about space exploration. So much new information...\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Listening to a podcast about space exploration. So much new information...\n",
      "INPUT TO VADER ['listen', 'to', 'a', 'podcast', 'about', 'space', 'exploration', '.', 'so', 'much', 'new', 'information', '...']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.2263}\n",
      "**************************************************\n",
      "\n",
      "Tweet: Would you like to watch the sunset at the park with me tonight?\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Would you like to watch the sunset at the park with me tonight?\n",
      "INPUT TO VADER ['would', 'you', 'like', 'to', 'watch', 'the', 'sunset', 'at', 'the', 'park', 'with', 'I', 'tonight', '?']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.3612}\n",
      "**************************************************\n",
      "\n",
      "Tweet: Listening to a podcast about productivity. It provides many useful tips.\n",
      "Expected: neutral; Predicted: positive\n",
      "\n",
      "VADER with verbose:\n",
      "\n",
      "INPUT SENTENCE Listening to a podcast about productivity. It provides many useful tips.\n",
      "INPUT TO VADER ['listen', 'to', 'a', 'podcast', 'about', 'productivity', '.', 'it', 'provide', 'many', 'useful', 'tip', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.4404}\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "misclassified_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
    "misclassified_tweets = {\"positive\": [], \"negative\": [], \"neutral\": []}\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    if gold[i] != all_vader_output[i]:  #classification is incorrect\n",
    "        true_label = gold[i]\n",
    "        predicted_label = all_vader_output[i]\n",
    "\n",
    "        misclassified_counts[true_label] += 1\n",
    "\n",
    "        misclassified_tweets[true_label].append((tweets[i], predicted_label))\n",
    "\n",
    "# Summary of misclassified words\n",
    "print(\"\\nMisclassification Summary\")\n",
    "print(\"-\" * 50)\n",
    "for sentiment, count in misclassified_counts.items():\n",
    "    print(f\"{sentiment.capitalize()} misclassified: {count}\")\n",
    "print(\"*\" * 50)\n",
    "for sentiment, errors in misclassified_tweets.items():\n",
    "    print(f\"\\nMisclassified {sentiment.capitalize()} Tweets\")\n",
    "    print(\"~\" * 50)\n",
    "\n",
    "    for tweet, predicted in errors: # print the tweets\n",
    "        print(f\"\\nTweet: {tweet}\")\n",
    "        print(f\"Expected: {sentiment}; Predicted: {predicted}\")\n",
    "        \n",
    "        print(\"\\nVADER with verbose:\") # see what vader assigns\n",
    "        run_vader(tweet, lemmatize=True, verbose=1)\n",
    "        print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(B) Error Analysis**\n",
    "In total, 0 positive tweets were misclassified, 4 negative tweets were misclassified as neutral, and 4 neutral were misclassified as positive.\n",
    "\n",
    "1. Misclassified Positive Tweets\n",
    "* All true-positive tweets are correctly identified.\n",
    "\n",
    "2. Misclassified Negative Tweets\n",
    "* All 4 true-negative tweets were misclassified as neutral. In all of them, we see that the `neu` score is 1.0, meaning that the classifier is absolutely sure the tweets convey a neutral tone. Therefore, its lexicon fails to associate the negative words in the sentence with a negative sentiment. For example, the tweet `\"The traffic today was unbearable. Took me whole two hours to get home.\"` clearly carries a negative sentiment, because of the word `unbearable`, which seems to be missing from VARDER's lexicon, possibly accounting for the fact that it classified the tweet as `neutral`. Similarly, the words `cold` and `tasteless` from the second tweet `\"The food at the restaurant was cold and tasteless. Never going back.\"` are also absent from VADER's lexicon, which leads to a misclassification as neutral despite the negative sentiment. The words `slow` and `cold` (again) indicating a negative sentiment in the third tweet `\"The service at the cafe was slow and the coffee was cold.\"` are missing as well, contributing to its neutral classification. Lastly, the terms `overpriced` and `mediocre` from the fourth tweet `\"The restaurant was overpriced and the food was mediocre.\"` are not found in VADER's lexicon, which likely resulted in the neutral classification of the tweet, even though it expresses a negative tone.\n",
    "\n",
    "3. Misclassified Neutral Tweets\n",
    "* As shown in the performance analysis, the classifier has the lowest performance score when it comes to neutral tweets. All 4 of the misclassified neutral tweets were classified as positive, suggesting that VADER tends to assign a sentiment even when the intended tone is neutral. A common issue observed is that VADER overweights slightly positive words, such as `innovation`, `exploration`, `useful`, and `like`. These words do not necessarily indicate strong positive emotion, but VADER assigns them a positive sentiment score. For example, in For example, in `\"Reading about the latest tech innovations. There have been a lot of new developments.\"`, the word `innovations` is factual rather than an expression of enthusiasm, yet it leads to a positive classification. The same is true for the word `exploration` in `\"Listening to a podcast about space exploration. So much new information...\"`, which also causes a misclassification. Moreover, VADER lacks context awareness, causing misinterpretations of the tone. In `\"Would you like to watch the sunset at the park with me tonight?\"`, the phrase `would you like` is not an expression of excitement but rather a neutral request, yet VADER assigns it a positive connotation. Similarly, in `\"Listening to a podcast about productivity. It provides many useful tips.\"`, the word `useful` describes a fact rather than carrying a positive sentiment, yet it considers the tweet to be positive.\n",
    "* In addition to the points made above, another possible reason for the observed misclassifications is that the threshold for classifying a tweet as positive is too lenient. Currently, any `compound` score greater than `0.0` results in a positive classification, even if the score is barely a positive number. This means that tweets with weakly positive words, even if the intended tone is neutral, are pushed towards the positive category. Raising the threshold slightly could help reduce these errors and better differentiate between truly neutral and positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Private\\Documents\\uni\\year3\\TextMiningGroup8\\Lab3\\airlinetweets\\airlinetweets\n"
     ]
    }
   ],
   "source": [
    "# path to folder\n",
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path().resolve()\n",
    "path_to_folder = Path.joinpath(cur_dir, r\"airlinetweets\")\n",
    "#path_to_folder = Path.joinpath(cur_dir, r\"airlinetweets/airlinetweets\")\n",
    "\n",
    "print(path_to_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@VirginAmerica Flying LAX to SFO and after looking at the awesome movie lineup I actually wish I was on a long haul.', 'positive')\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "import os\n",
    "\n",
    "data = []\n",
    "categories = [\"positive\", \"negative\", \"neutral\"]\n",
    "for c in categories:\n",
    "    category_path = os.path.join(path_to_folder, c)\n",
    "\n",
    "    for txt_file in os.listdir(category_path):\n",
    "        txt_file_path = os.path.join(category_path, txt_file)\n",
    "        \n",
    "        with open(txt_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            tweet = file.read().strip()\n",
    "            data.append((tweet, c))\n",
    "\n",
    "# checking random example\n",
    "print(data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Basic VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.64      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = set()\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: VADER with Lemmatized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# perform sentiment analysis with VADER on each tweet\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet, label \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m---> 11\u001b[0m     vader_output \u001b[38;5;241m=\u001b[39m run_vader(tweet, lemmatize\u001b[38;5;241m=\u001b[39mto_lemmatize, parts_of_speech_to_consider\u001b[38;5;241m=\u001b[39mpos) \u001b[38;5;66;03m# use run_vader function\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     vader_label \u001b[38;5;241m=\u001b[39m vader_output_to_label(vader_output) \u001b[38;5;66;03m# use the predefined function above to get the labels based on scores\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     airline_tweets\u001b[38;5;241m.\u001b[39mappend(tweet)\n",
      "Cell \u001b[1;32mIn[44], line 20\u001b[0m, in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_vader\u001b[39m(textual_unit, \n\u001b[0;32m      2\u001b[0m               lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m      3\u001b[0m               parts_of_speech_to_consider\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m               verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Run VADER on a sentence from spacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    :return: vader output dict\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(textual_unit)\n\u001b[0;32m     22\u001b[0m     input_to_vader \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\spacy\\ml\\tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m ParserStepModel(\n\u001b[0;32m     35\u001b[0m         X,\n\u001b[0;32m     36\u001b[0m         model\u001b[38;5;241m.\u001b[39mlayers,\n\u001b[0;32m     37\u001b[0m         unseen_classes\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munseen_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     38\u001b[0m         train\u001b[38;5;241m=\u001b[39mis_train,\n\u001b[0;32m     39\u001b[0m         has_upper\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Private\\anaconda3\\Lib\\site-packages\\thinc\\layers\\linear.py:38\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     36\u001b[0m W \u001b[38;5;241m=\u001b[39m cast(Floats2d, model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     37\u001b[0m b \u001b[38;5;241m=\u001b[39m cast(Floats1d, model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 38\u001b[0m Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgemm(X, W, trans2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY: OutT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InT:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: VADER with Only Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.20      0.33      1750\n",
      "     neutral       0.40      0.89      0.55      1515\n",
      "    positive       0.67      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.64      0.51      0.47      4755\n",
      "weighted avg       0.65      0.50      0.46      4755\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = {\"ADJ\", \"JJ\", \"JJR\", \"JJS\"}  # include universal as well as Penn Treebank tags for adjective\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: VADER with Lemmatized Text of Only Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.20      0.33      1750\n",
      "     neutral       0.40      0.89      0.55      1515\n",
      "    positive       0.67      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.64      0.51      0.47      4755\n",
      "weighted avg       0.65      0.50      0.46      4755\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = {\"ADJ\", \"JJ\", \"JJR\", \"JJS\"}  # include universal as well as Penn Treebank tags for adjective\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: VADER with Only Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.14      0.23      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.53      0.35      0.42      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.44      0.39      4755\n",
      "weighted avg       0.55      0.42      0.38      4755\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = {\"NOUN\", \"NN\", \"NNS\"}  # include universal as well as Penn Treebank tags for nouns (proper nouns not included)\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: VADER with Lemmatized Text of Only Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.15      0.25      1750\n",
      "     neutral       0.36      0.81      0.50      1515\n",
      "    positive       0.52      0.34      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.44      0.39      4755\n",
      "weighted avg       0.54      0.42      0.38      4755\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = {\"NOUN\", \"NN\", \"NNS\"}  # include universal as well as Penn Treebank tags for nouns (proper nouns not included)\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: VADER with Only Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.29      0.42      1750\n",
      "     neutral       0.38      0.81      0.52      1515\n",
      "    positive       0.57      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.46      4755\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = False \n",
    "pos = {\"VERB\", \"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}  # include universal as well as Penn Treebank tags for verbs\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8: VADER with Lemmatized Text of Only Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.29      0.42      1750\n",
      "     neutral       0.38      0.78      0.51      1515\n",
      "    positive       0.57      0.36      0.44      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.57      0.48      0.46      4755\n",
      "weighted avg       0.58      0.47      0.46      4755\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      4755\n"
     ]
    }
   ],
   "source": [
    "airline_tweets = []\n",
    "airline_all_vader_output = []\n",
    "airline_gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = {\"VERB\", \"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}  # include universal as well as Penn Treebank tags for verbs\n",
    "\n",
    "# perform sentiment analysis with VADER on each tweet\n",
    "for tweet, label in data:\n",
    "    vader_output = run_vader(tweet, lemmatize=to_lemmatize, parts_of_speech_to_consider=pos) # use run_vader function\n",
    "    vader_label = vader_output_to_label(vader_output) # use the predefined function above to get the labels based on scores\n",
    "    \n",
    "    airline_tweets.append(tweet)\n",
    "    airline_all_vader_output.append(vader_label)\n",
    "    airline_gold.append(label)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(airline_gold, airline_all_vader_output))\n",
    "\n",
    "# calculate the micro precision, recall and f1 score using the corresponding functions since\n",
    "# micro averages are not part of classification report\n",
    "precision_micro = precision_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "recall_micro = recall_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "f1_score_micro = f1_score(y_true=airline_gold, y_pred=airline_all_vader_output, average='micro')\n",
    "\n",
    "print(f\"{'micro avg':>12}{precision_micro:>11.2f}{recall_micro:>10.2f}{f1_score_micro:>10.2f}{len(airline_tweets):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison**\n",
    "\n",
    "#### Lemmatization:\n",
    "TO BE ADDED\n",
    "\n",
    "#### Importance of POS for Sentiment Analysis:\n",
    "TO BE ADDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "df = pd.DataFrame(data, columns=[\"tweet\", \"sentiment\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"tweet\"], df[\"sentiment\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic settings (TF-IDF representation, min_df=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.93      0.84       328\n",
      "     neutral       0.88      0.70      0.78       296\n",
      "    positive       0.87      0.85      0.86       327\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training with default settings (TF-IDF representation, min_df=2)\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n",
    "\n",
    "# using Multinomial NB because it is good for txt classification\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorizer, y_train)\n",
    "y_pred = model.predict(X_test_vectorizer)\n",
    "\n",
    "print(\"Basic settings (TF-IDF representation, min_df=2)\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words representation ('airline_count')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.93      0.87       328\n",
      "     neutral       0.86      0.71      0.78       296\n",
      "    positive       0.85      0.86      0.85       327\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIFFERENT SETTINGS 1\n",
    "# with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count')\n",
    "\n",
    "# Bag of words representation ('airline_count')\n",
    "# for BoW use CountVectorizer\n",
    "vectorizer_count = CountVectorizer(min_df=2)  \n",
    "X_train_counter = vectorizer_count.fit_transform(X_train)\n",
    "X_test_counter = vectorizer_count.transform(X_test)\n",
    "\n",
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_counter, y_train)\n",
    "y_pred_count = model_count.predict(X_test_counter)\n",
    "\n",
    "print(\"Bag of words representation ('airline_count')\")\n",
    "print(classification_report(y_test, y_pred_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vs BoW\n",
    "\n",
    "To compare TF-IDF and BoW results, we analyze the output of the classification_report. First of all, the overall accuracy is slightly better with BoW than with TF-IDF (84% and 83%, respectively). Furthermore, if we compare by category, the results are very similar for the neutral and positive categories (TF-IDF has slightly better precision, but the difference is very small). \n",
    "\n",
    "Neutral category: \n",
    "- TF-IDF: Precision 0.88, Recall 0.70, F1-score 0.78; \n",
    "- BoW: Precision 0.86, Recall 0.71, F1-score 0.78\n",
    "\n",
    "Positive category:\n",
    "- TF-IDF: Precision 0.87, Recall 0.85, F1-score 0.86;\n",
    "- BoW: Precision 0.85, Recall 0.86, F1-score 0.85\n",
    "\n",
    "--> TF-IDF has slighly better precision but the difference is very small\n",
    "\n",
    "Negative category:\n",
    "- TF-IDF: Precision 0.77, Recall 0.93, F1-score 0.84;\n",
    "- BoW: Precision 0.81, Recall 0.93, F1-score 0.87\n",
    "\n",
    "--> for the negative category, BoW performs better- it results in higher precision (0.81 vs. 0.77) while maintaining the same recall (0.93). From this, we can conclude that BoW produces fewer false positives. The overall higher accuracy of BoW can be explained by its better performance on negative tweets.\n",
    "\n",
    "\n",
    "These differences are due to the specificity of both text vectorization techniques:\n",
    "1) BoW counts how many times each word appears in the tweet (ignoring grammar, order, meaning). It is a simpler approach and can be effective for capturing common words. However, it does not consider the importance of words, meaning that even frequent but unimportant words (such as \"the\") can receive high weight.\n",
    "2) TF-IDF is similar to BoW but it adjusts for word importance. Instead of just taking raw word counts, it gives weight to words that are more important in a document. It assigns weights based on term frequency (TF) and inverse document frequency (IDF), where IDF penalizes common words, reducing their influence in classification.\n",
    "\n",
    "In the case of our tweets, BoW slightly outperformed TF-IDF because tweets contain strong, frequent words that aid classification. Examples of frequent negative words include \"bad\", \"worst\", \"delayed\", \"cancelled\", \"rude\", \"late\", \"horrible\", and \"never\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with TF-IDF, min_df=2\n",
      "TF-IDF classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.93      0.84       328\n",
      "     neutral       0.88      0.70      0.78       296\n",
      "    positive       0.87      0.85      0.86       327\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      "\n",
      "\n",
      "Training with BoW, min_df=2\n",
      "BoW classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.93      0.87       328\n",
      "     neutral       0.86      0.71      0.78       296\n",
      "    positive       0.85      0.86      0.85       327\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.83       951\n",
      "\n",
      "\n",
      "Training with TF-IDF, min_df=5\n",
      "TF-IDF classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86       328\n",
      "     neutral       0.83      0.74      0.78       296\n",
      "    positive       0.86      0.82      0.84       327\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n",
      "\n",
      "Training with BoW, min_df=5\n",
      "BoW classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88       328\n",
      "     neutral       0.84      0.73      0.78       296\n",
      "    positive       0.84      0.83      0.84       327\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.83       951\n",
      "\n",
      "\n",
      "Training with TF-IDF, min_df=10\n",
      "TF-IDF classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86       328\n",
      "     neutral       0.81      0.75      0.78       296\n",
      "    positive       0.86      0.80      0.83       327\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.82      0.82       951\n",
      "weighted avg       0.83      0.83      0.82       951\n",
      "\n",
      "\n",
      "Training with BoW, min_df=10\n",
      "BoW classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.93      0.88       328\n",
      "     neutral       0.82      0.73      0.77       296\n",
      "    positive       0.84      0.81      0.82       327\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.82      0.82       951\n",
      "weighted avg       0.83      0.83      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIFFERENT SETTINGS 2\n",
    "# with respect to the frequency threshold (min_df)\n",
    "\n",
    "# values of document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "min_df_values = [2, 5, 10]\n",
    "\n",
    "# train a model with respect to each frequency threshold\n",
    "for min_df in min_df_values:\n",
    "    print(f\"\\nTraining with TF-IDF, min_df={min_df}\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=min_df)\n",
    "    X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorizer = vectorizer.transform(X_test)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vectorizer, y_train)\n",
    "    y_pred = model.predict(X_test_vectorizer)\n",
    "\n",
    "    print(\"TF-IDF classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nTraining with BoW, min_df={min_df}\")\n",
    "    \n",
    "    vectorizer_bow = CountVectorizer(min_df=min_df)\n",
    "    X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "    X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "    model_bow = MultinomialNB()\n",
    "    model_bow.fit(X_train_bow, y_train)\n",
    "    y_pred_bow = model_bow.predict(X_test_bow)\n",
    "\n",
    "    print(\"BoW classification report:\")\n",
    "    print(classification_report(y_test, y_pred_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency threshold experiment results\n",
    "\n",
    "### Classification report results\n",
    "\n",
    "1) TF-IDF vs. BoW (min_df=2)\n",
    "\n",
    "As mentioned in the result analysis in the previous section, the accuracy for BoW is slightly better at 84% compared to TF-IDF’s 83%. \n",
    "\n",
    "Per category:\n",
    "- Negative tweets: BoW outperforms TF-IDF with higher precision (0.81 vs 0.77) and the same recall (0.93) --> fewer false positives\n",
    "- Neutral tweets: TF-IDF has slightly higher precision (0.88 vs 0.86) and lower recall (0.70 vs 0.71) but the difference is small\n",
    "- Positive tweets: TF-IDF has slightly better precision (0.87 vs 0.85) but BoW has higher recall (0.86 vs 0.85) but the difference is small also here\n",
    "\n",
    "As pointed out in the previous comparison, BoW slightly outperforms TF-IDF due to better performance on negative tweets (see the details above).\n",
    "\n",
    "2) TF-IDF vs. BoW (min_df=5)\n",
    "\n",
    "The overall accuracy did not change here. TF-IDF has 83%, while BoW scores 84%.\n",
    "\n",
    "Per category:\n",
    "- Negative tweets: BoW outperforms TF-IDF with higher precision (0.83 vs 0.80) and recall (0.94 vs 0.92)\n",
    "- Neutral tweets: results are very close. TF-IDF performs slightly better with precision (0.83 vs 0.84) but the recall is lower (0.74 vs 0.73)\n",
    "- Positive tweets: results are very close. BoW has slightly better recall (0.83 vs 0.82), the precision is slightly better in TF-IDF (0.86 vs 0.84)\n",
    "\n",
    "In this case, both TF-IDF and BoW resulted in identical accuracy as before. BoW is again slightly better at handling negative tweets. The performance is closer for the neutral and positive categories. The frequency threshold (min_df=5) has not notecably impacted performance compared to min_df=2.\n",
    "\n",
    "3) TF-IDF vs. BoW (min_df=10)\n",
    "Both models show 83% accuracy. Compared to previous experiments, the accuracy of BoW has decreased by 1%, while the accuracy of TF-IDF remains the same.\n",
    "\n",
    "Per category:\n",
    "- Negative tweets: BoW has higher precision (0.82 vs. 0.80) and higher recall (0.93 vs. 0.92)\n",
    "- Neutral tweets: TF-IDF has slightly better recall (0.75 vs. 0.73), but BoW has better precision (0.82 vs. 0.81)\n",
    "- Positive tweets: TF-IDF has slightly lower recall (0.80 vs. 0.81) and slightly better precision (0.86 vs 0.84)\n",
    "\n",
    "In this case, both TF-IDF and BoW resulted in identical accuracy of 83%. BoW remains slightly better at handling negative tweets, while the performance for neutral and positive categories is very close. The frequency threshold min_df=10 has had a minimal impact on overall performance compared to min_df=5 and min_df=2.\n",
    "\n",
    "### Impact of increasing min_df\n",
    "\n",
    "Increasing min_df: increasing min_df from 2 to 10 removes rare words and the vocabulary size becomes smaller. BoW is more affected because it relies on raw word counts compared to TF-IDF, which adjusts more easily by weighting important words, so the accuracy of the results does not differ. In conclusion, removing most of the rare words has limited impact on the classification performance, which means that these tweets can be classified based on set of frequently occurring words rather than relying on rare words. Sentiment of airline tweets is determined by commonly used terms. Even with min_df = 10, the model maintains a similar accuracy --> `the key words influencing sentiment classification are common across many tweets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "negative 1490.0 @\n",
      "negative 1383.0 united\n",
      "negative 1233.0 .\n",
      "negative 734.0 to\n",
      "negative 568.0 i\n",
      "negative 538.0 the\n",
      "negative 451.0 a\n",
      "negative 414.0 ``\n",
      "negative 394.0 flight\n",
      "negative 393.0 ?\n",
      "negative 374.0 you\n",
      "negative 369.0 !\n",
      "negative 361.0 and\n",
      "negative 325.0 my\n",
      "negative 322.0 on\n",
      "negative 315.0 for\n",
      "negative 314.0 #\n",
      "negative 294.0 is\n",
      "negative 257.0 in\n",
      "negative 216.0 n't\n",
      "negative 210.0 of\n",
      "negative 196.0 it\n",
      "negative 194.0 that\n",
      "negative 188.0 your\n",
      "negative 171.0 have\n",
      "negative 167.0 not\n",
      "negative 161.0 me\n",
      "negative 157.0 with\n",
      "negative 153.0 was\n",
      "negative 153.0 ''\n",
      "negative 152.0 no\n",
      "negative 142.0 at\n",
      "negative 129.0 this\n",
      "negative 122.0 's\n",
      "negative 114.0 service\n",
      "negative 114.0 do\n",
      "negative 111.0 :\n",
      "negative 110.0 from\n",
      "negative 104.0 now\n",
      "negative 99.0 get\n",
      "negative 98.0 be\n",
      "negative 97.0 virginamerica\n",
      "negative 92.0 delayed\n",
      "negative 90.0 we\n",
      "negative 89.0 been\n",
      "negative 89.0 are\n",
      "negative 88.0 plane\n",
      "negative 88.0 just\n",
      "negative 88.0 cancelled\n",
      "negative 86.0 an\n",
      "negative 85.0 customer\n",
      "negative 84.0 but\n",
      "negative 83.0 time\n",
      "negative 78.0 they\n",
      "negative 78.0 so\n",
      "negative 77.0 why\n",
      "negative 74.0 ;\n",
      "negative 73.0 ...\n",
      "negative 72.0 hours\n",
      "negative 72.0 'm\n",
      "negative 71.0 what\n",
      "negative 71.0 bag\n",
      "negative 70.0 -\n",
      "negative 67.0 can\n",
      "negative 66.0 http\n",
      "negative 66.0 &\n",
      "negative 64.0 still\n",
      "negative 63.0 will\n",
      "negative 62.0 has\n",
      "negative 62.0 gate\n",
      "negative 61.0 did\n",
      "negative 60.0 how\n",
      "negative 60.0 amp\n",
      "negative 59.0 airline\n",
      "negative 59.0 2\n",
      "negative 58.0 hour\n",
      "negative 58.0 again\n",
      "negative 57.0 when\n",
      "negative 57.0 out\n",
      "negative 57.0 our\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "neutral 1403.0 @\n",
      "neutral 583.0 to\n",
      "neutral 514.0 .\n",
      "neutral 502.0 ?\n",
      "neutral 439.0 i\n",
      "neutral 300.0 jetblue\n",
      "neutral 287.0 the\n",
      "neutral 272.0 :\n",
      "neutral 271.0 a\n",
      "neutral 263.0 united\n",
      "neutral 262.0 southwestair\n",
      "neutral 256.0 you\n",
      "neutral 246.0 ``\n",
      "neutral 245.0 on\n",
      "neutral 232.0 #\n",
      "neutral 222.0 flight\n",
      "neutral 192.0 americanair\n",
      "neutral 189.0 for\n",
      "neutral 187.0 my\n",
      "neutral 174.0 in\n",
      "neutral 171.0 http\n",
      "neutral 169.0 !\n",
      "neutral 165.0 is\n",
      "neutral 162.0 usairways\n",
      "neutral 154.0 can\n",
      "neutral 142.0 and\n",
      "neutral 139.0 from\n",
      "neutral 139.0 's\n",
      "neutral 128.0 it\n",
      "neutral 120.0 of\n",
      "neutral 115.0 me\n",
      "neutral 115.0 do\n",
      "neutral 110.0 have\n",
      "neutral 80.0 that\n",
      "neutral 78.0 at\n",
      "neutral 77.0 with\n",
      "neutral 75.0 virginamerica\n",
      "neutral 74.0 what\n",
      "neutral 74.0 this\n",
      "neutral 73.0 get\n",
      "neutral 69.0 any\n",
      "neutral 67.0 are\n",
      "neutral 67.0 -\n",
      "neutral 66.0 be\n",
      "neutral 65.0 ''\n",
      "neutral 64.0 our\n",
      "neutral 63.0 please\n",
      "neutral 62.0 will\n",
      "neutral 61.0 flights\n",
      "neutral 60.0 we\n",
      "neutral 60.0 if\n",
      "neutral 60.0 help\n",
      "neutral 58.0 there\n",
      "neutral 57.0 n't\n",
      "neutral 56.0 need\n",
      "neutral 56.0 just\n",
      "neutral 56.0 )\n",
      "neutral 49.0 how\n",
      "neutral 48.0 when\n",
      "neutral 48.0 ...\n",
      "neutral 47.0 your\n",
      "neutral 47.0 (\n",
      "neutral 46.0 dm\n",
      "neutral 45.0 out\n",
      "neutral 45.0 ;\n",
      "neutral 42.0 was\n",
      "neutral 40.0 or\n",
      "neutral 40.0 now\n",
      "neutral 39.0 “\n",
      "neutral 39.0 us\n",
      "neutral 39.0 fleet\n",
      "neutral 39.0 fleek\n",
      "neutral 39.0 about\n",
      "neutral 39.0 &\n",
      "neutral 38.0 so\n",
      "neutral 38.0 know\n",
      "neutral 36.0 ”\n",
      "neutral 36.0 thanks\n",
      "neutral 36.0 not\n",
      "neutral 36.0 hi\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "positive 1343.0 @\n",
      "positive 1062.0 !\n",
      "positive 763.0 .\n",
      "positive 467.0 you\n",
      "positive 418.0 the\n",
      "positive 417.0 to\n",
      "positive 324.0 for\n",
      "positive 314.0 southwestair\n",
      "positive 312.0 i\n",
      "positive 300.0 #\n",
      "positive 295.0 jetblue\n",
      "positive 290.0 thanks\n",
      "positive 255.0 united\n",
      "positive 246.0 thank\n",
      "positive 242.0 ``\n",
      "positive 216.0 a\n",
      "positive 202.0 and\n",
      "positive 181.0 flight\n",
      "positive 173.0 :\n",
      "positive 169.0 americanair\n",
      "positive 152.0 my\n",
      "positive 147.0 on\n",
      "positive 133.0 great\n",
      "positive 132.0 usairways\n",
      "positive 128.0 your\n",
      "positive 125.0 in\n",
      "positive 124.0 it\n",
      "positive 113.0 of\n",
      "positive 108.0 so\n",
      "positive 108.0 is\n",
      "positive 106.0 me\n",
      "positive 101.0 with\n",
      "positive 92.0 )\n",
      "positive 91.0 service\n",
      "positive 88.0 was\n",
      "positive 86.0 virginamerica\n",
      "positive 82.0 at\n",
      "positive 76.0 love\n",
      "positive 75.0 this\n",
      "positive 73.0 http\n",
      "positive 72.0 are\n",
      "positive 67.0 customer\n",
      "positive 66.0 that\n",
      "positive 66.0 guys\n",
      "positive 62.0 much\n",
      "positive 61.0 best\n",
      "positive 60.0 just\n",
      "positive 59.0 be\n",
      "positive 58.0 have\n",
      "positive 58.0 -\n",
      "positive 56.0 awesome\n",
      "positive 56.0 ;\n",
      "positive 56.0 's\n",
      "positive 55.0 good\n",
      "positive 54.0 from\n",
      "positive 52.0 all\n",
      "positive 51.0 we\n",
      "positive 48.0 out\n",
      "positive 47.0 got\n",
      "positive 45.0 amazing\n",
      "positive 44.0 up\n",
      "positive 44.0 airline\n",
      "positive 43.0 time\n",
      "positive 43.0 n't\n",
      "positive 41.0 will\n",
      "positive 40.0 today\n",
      "positive 40.0 crew\n",
      "positive 39.0 &\n",
      "positive 38.0 get\n",
      "positive 37.0 help\n",
      "positive 36.0 us\n",
      "positive 36.0 our\n",
      "positive 35.0 made\n",
      "positive 35.0 flying\n",
      "positive 34.0 very\n",
      "positive 34.0 fly\n",
      "positive 34.0 do\n",
      "positive 34.0 back\n",
      "positive 34.0 ...\n",
      "positive 33.0 now\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out() # had to change this line of code to _out(), because in newer version of scikit-learn it was removed\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "#important_features_per_class(airline_vec, clf)\n",
    "\n",
    "airline_tweets = load_files(\n",
    "    \"airlinetweets\",\n",
    "    categories=[\"neutral\", \"positive\", \"negative\"],\n",
    "    encoding=\"utf-8\",\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    "                            )\n",
    "tweets = airline_tweets.data\n",
    "labels = airline_tweets.target\n",
    "label_names = airline_tweets.target_names\n",
    "labels = [label_names[label] for label in labels]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweets, labels, train_size=0.8, random_state=0)\n",
    "\n",
    "# x_train = [\" \".join(word_tokenize(text)) for text in x_train]  # we can use this if we want to remove punctuation and maybe improve our model\n",
    "# x_test = [\" \".join(word_tokenize(text)) for text in x_test]\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize) # remove tokenizer argument if we want to remove punctuation, and uncomment the lines above\n",
    "train_counts = vectorizer.fit_transform(x_train)\n",
    "test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, y_train)\n",
    "\n",
    "\n",
    "important_features_per_class(vectorizer, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Features\n",
    "\n",
    "#### Negative Class\n",
    "**Expected Words:** `\"delay\"`, `\"cancelled\"`, `\"bad\"`, `\"late\"`,`\"rude\"` \n",
    "\n",
    "**Why?** People usually express frustration on Twitter when their flight is delayed, canceled, or when they experience poor service.\n",
    "\n",
    "#### Neutral Class\n",
    "**Expected Words:** `\"flight\"`, `\"gate\"`, `\"airport\"`  \n",
    "\n",
    "**Why?** Neutral tweets are often just factual, mentioning general travel related words without expressing strong emotions, this also makes it harder to predict neutral words.\n",
    "\n",
    "#### Positive Class\n",
    "**Expected Words:**  `\"helpful\"`, `\"friendly\"`, `\"best\"`, `\"great\"`, `\"amazing\"`,    \n",
    "\n",
    "**Why?** Happy customers tend to express gratitude and enthusiasm when they receive good service.\n",
    "\n",
    "---\n",
    "\n",
    "### Unexpected Features \n",
    "Some unexpected features appeared in the output, such as punctuation and symbols like `\"@\"`, `\".\"`, `\"!\"`, `\"?\"`, and `\"#\"`, which are structural elements of tweets but do not carry sentiment. Additionally, common stopwords such as `\"the\"`, `\"a\"`, `\"is\"`, `\"with\"`, and `\"that\"` were surprisingly ranked high despite being function words that frequently appear in all tweets without having meaningful sentiment. Furthermore, numbers and generic words like `\"2\"`, `\"hours\"`, and `\"http\"` were unexpected. While `\"hour\"` and `\"hours\"` may sometimes indicate delays, numerical values and URLs generally do not convey sentiment on their own.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Words That Should Be Kept Or Removed\n",
    "To improve the classification model, we should remove words that don’t add sentiment meaning and keep those that do.\n",
    "\n",
    "#### Words to Remove that have No Sentiment Meaning\n",
    "- **Punctuation**: `\".\"`, `\"!\"`, `\"?\"`, `\"...\"`, `\"@\"`, `\"#\"`  \n",
    "- **Stopwords**: `\"the\"`, `\"to\"`, `\"a\"`, `\"is\"`, `\"in\"`, `\"with\"`, `\"that\"`, `\"be\"`  \n",
    "- **Mentions & URLs**: `\"@\"`, `\"http\"`  \n",
    "- **Symbols**: `\"&\"`, `\"-\"`, `\":\"`, `\"''\"`\n",
    "\n",
    "#### Words to Keep that have Strong Sentiment Meaning\n",
    "- **Negative Sentiment:** `\"worst\"`, `\"cancelled\"`, `\"delayed\"`, `\"rude\"`, `\"bad\"`, `\"no\"`  \n",
    "- **Neutral Words:** `\"ticket\"`, `\"service\"`, `\"airport\"`, `\"flight\"`, `\"gate\"`  \n",
    "- **Positive Sentiment:** `\"helpful\"`, `\"best\"`, `\"great\"`, `\"friendly\"`, `\"good\"`, `\"awesome\"`, `\"amazing\"`  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Model Improvement \n",
    "To make the classification more accurate, we can apply the following techniques:\n",
    "\n",
    "1. **Remove Stopwords**: Use `stop_words=\"english\"` in `CountVectorizer` to filter out common words that don’t add meaning.  \n",
    "2. **Exclude Punctuation** \n",
    "4. **Apply Stemming or Lemmatization**: Convert words to their root form (e.g., `\"delayed\"` → `\"delay\"`, `\"cancelled\"` → `\"cancel\"`).  \n",
    "\n",
    "By applying these refinements, we can improve our Naive Bayes model's ability to classify tweets more accurately and meaningfully.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
